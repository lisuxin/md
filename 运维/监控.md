[toc]

# Prometheus

使用golang编写实现，一般与Grafana配合使用

## 架构

![image-20250505171450950](../typoratuxiang/yunwei/prometheus/p1.png)

* Prometheus server：Prometheus服务的端
  * Retrieval：收集器
  * TSDB：时序数据库
  * HTTP server：web服务
* Node底层
  * HDD、SSD：底层可以运行在底层物理机云上都可以HDD机械硬盘、SSD
* Prometheus targets客户端
  * 在被Prometheus部署的各种exporter(客户端)；监控什么就用什么exporter
* pull metrics：拉取服务端各项监控指标，就是需要监控什么就去拉取什么的exporter
* push alerts：发送告警
* Alertmanager：告警插件
  * pagerduty：告警平台：onealert
* promQL：查询语句
  * Prometheus webUI：前端页面查看数据
  * Grafana：Prometheus与Grafana联合数据展示
  * API：API接口、接口提供数据访问
* Pushgateway：不方便使用exporter安装客户端的使用
  * short-lived jobs：客户端主动发信息给push网关、Pushgateway发送数据给服务端
* Service discovery：自动发现

## 部署

1. 检查各个节点的时间同步

   1. 安装ntp`yum -y install ntp`
   2. 设置开机自启`systemctl start ntpd、systemctl enable ntpd`
   3. 手动校准时间、使用阿里云`ntpdate -u ntp.aliyun.com`

2. 下载解压配置

   1. 使用容器的方式运行、编写k8s.yml

      ```yaml
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: prometheus-deployment
      spec:
        selector:
          matchLabels:
            app: prometheus
        template:
          metadata:
            labels:
              app: prometheus
          spec:
            nodeSelector:
              kubernetes.io/hostname: k8s-node-96
            containers:
              - name: prometheus
                image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/quay.io/prometheus/prometheus:v3.2.1
                resources:
                  limits:
                    memory: "2Gi"
                    cpu: "1"
                ports:
                  - containerPort: 9090
                    name: prometheus-port
                volumeMounts:
                  - name: prometheus-data
                    mountPath: /prometheus
            volumes:
              - name: prometheus-data
                hostPath:
                  path: /opt/prometheus
                  type: DirectoryOrCreate
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: prometheus-service
      spec:
        type: NodePort
        selector:
          app: prometheus
        ports:
          - name: prometheus-port
            port: 9090
            targetPort: 9090
            nodePort: 30085
      ```

   2. 使用二进制文件安装

      1. 下载压缩包、解压
      2. 目录
         * prometheus：服务端命令
         * prometheus.yml：配置文件
      3. 查看版本信息`/export/server/prometheus-3.4.0/prometheus --version`
      4. 启动（进入目录）
         * 前台启动：`/export/server/prometheus-3.4.0/prometheus`
         * 后台运行：`nohup /export/server/prometheus-3.4.0/prometheus &>> /export/server/prometheus-3.4.0/prometheus.log &`重定向到日志中

3. 启动`kubectl apply -f ./k8s.yaml`

4. 访问web UI：地址加端口

5. 查看所有promQL`http://192.168.31.98:9090/metrics`

6. 服务端命令

   * `--config.file="prometheus.yml"`：指定配置文件，默认是当前目录下在的`prometheus.yml`
   * `--web.listen-address="0.0.0.0:9090"`：前端 web 页面，端口和监听的地址。如果想增加访问认证可以用 ngx
   * `--web.max-connections=512`并发连接数
   * `--storage.tsdb.path="data/"`指定 tsdb 数据存放目录．
   * `--log.levet=info`日志级别， prometheus 日志默认输出到屏幕（标准输出）
   * `--log.format=logfmt`日志格式。logfmt 默认格式。json 格式（日志收集的时候使用）

7. （二进制安装）使用systemctl管理

   1. 在`/etc/systemd/system/`目录下创建`prometheus.service`

      ```bash
      # 定义服务的基本信息部分
      [Unit]
      # 描述该服务的名称（在 systemctl status 和日志中显示）
      Description=prometheus service
      # 指定本服务在哪个目标（target）之后启动，network.target 表示在网络准备好之后启动
      After=network.target
      # 定义服务运行时的行为
      [Service]
      # 指定运行该服务的用户（建议不要使用 root 用户，应创建专用账户 prometheus）
      User=prometheus
      # 指定运行该服务的用户组
      Group=prometheus
      # 服务类型为 simple，表示 ExecStart 启动的主进程就是服务本身
      Type=simple
      # 启动命令：指定 Prometheus 的可执行文件路径，并传入相关参数
      # --config.file：指定配置文件路径
      # --web.listen-address：设置 Prometheus Web 界面监听地址和端口（默认是 9090）
      # --web.max-connections：限制最大连接数为 512
      ExecStart=/export/server/prometheus-3.4.0/prometheus \
          --config.file="/export/server/prometheus-3.4.0/prometheus.yml" \
          --web.listen-address="0.0.0.0:9090" \
          --web.max-connections=512 
      # 设置标准输出（stdout）的日志输出方式：
      # file: 将日志写入指定文件（支持轮转，但不自动清理），这里记录到 prometheus.log 中
      StandardOutput=file:/export/server/prometheus-3.4.0/prometheus.log
      # KillMode=process 表示当服务停止时，只杀死主进程，而不是整个进程组
      KillMode=process
      # 定义服务安装行为（即启用开机自启时生效）
      [Install]
      # WantedBy=multi-user.target 表示在多用户模式下启用该服务（即正常启动系统后运行）
      WantedBy=multi-user.target
      ```

   2. 重新加载systemd配置`systemctl daemon-reload`

   3. 设置开机自启`systemctl enable prometheus`

   4. 启动`systemctl start prometheus`

   5. 查看服务`systemctl status prometheus`

   6. 服务重启`systemctl restart prometheus`

   7. 关闭服务`systemctl stop prometheus`

### 配置文件

* `prometheus.yml`

  ```yaml
  # 全局配置
  global:
    scrape_interval: 15s # 获取信息的间隔时间
    evaluation_interval: 15s # # 执行一些对应报警规则（采集报警时间）
    # scrape_timeout is set to the global default (10s).采集数据的超时时间
  
  alerting: # 告警管理器配置部分，用于处理来自Prometheus的告警信息。
    alertmanagers: # 定义Alertmanager实例的位置列表。
      - static_configs: # 使用静态配置指定Alertmanager的位置。
          - targets: # 列表中的每个元素都是一个Alertmanager实例的地址。
            # 示例：- "alertmanager.example.com:9093" 
  
  rule_files: # 规则文件加载配置，定义包含告警和记录规则的文件路径。
    # 示例：- "/path/to/your/rules.yml"
  
  scrape_configs: # 配置数据采集（客户端）
    - job_name: "prometheus" # 采集任务的名称，哪些机器或哪些服务的哪些指标
      static_configs: # 静态配置文件、指定被采集的对象、修改后需要重启prometheus
        - targets: ["localhost:9090"] # ip加端口
          labels:
            app: "prometheus"
      file_sd_configs: # 动态配置文件、动态读取文件内容，然后进行采集、实时监控修改动态配置文件不需要重启prometheus
        - files: # 指定包含目标信息的文件列表。
          - your_dynamic_config_file.json # 示例：替换为实际的动态配置文件路径。
          refresh_interval: 5s # 读取配置文件的时间间隔
  ```
  
* 动态配置文件格式`your_dynamic_config_file.json`

  ```json
  [
      {
          "targets":
          - "ip:端口"
          - "ip:端口"
      }
  ]
  ```

## exporter

1. 下载各种exporter，然后解压运行（或者使用容器形式运行）

   * （下载解压安装的）使用systemctl管理、在`/etc/systemd/system/`目录下创建`.service`文件

     * 解压最好解压在prometheus安装目录下

   * 在下载解压的exporter目录下有一个`服务_xporter`名称做一个软连接(让在任何位置都可以使用`服务_xporter`)

     ```bash
     ln -s /服务_exporter /usr/bin/
     ```

2. 配置`prometheus.yml`文件

   * 使用静态配置

     ```yaml
     scrape_configs: # 配置数据采集（客户端）
       - job_name: "prometheus" # 采集任务的名称，哪些机器或哪些服务的哪些指标
         static_configs: # 静态配置文件、指定被采集的对象、修改后需要重启prometheus
           - targets: ["localhost:9090"] # ip加端口
             labels:
               app: "prometheus"
     ```

     1. 配置完成需要重启prometheus、`systemctl restart prometheus`、以后每次加入或减少exporter都需要重启prometheus

   * 使用动态配置

     ```yaml
     scrape_configs: # 配置数据采集（客户端）
       - job_name: "prometheus" # 采集任务的名称，哪些机器或哪些服务的哪些指标
         file_sd_configs: # 动态配置文件、动态读取文件内容，然后进行采集、实时监控修改动态配置文件不需要重启prometheus
           - files: # 指定包含目标信息的文件列表。
             - your_dynamic_config_file.json # 示例：替换为实际的动态配置文件路径。
             refresh_interval: 5s # 读取配置文件的时间间隔
     ```

     1. 编写json文件

        ```json
        [
            {
                "targets":
                - "ip:端口"
                - "ip:端口"
            }
        ]
        ```

     2. 配置完成需要重启prometheus、`systemctl restart prometheus`、以后只需要修改json文件、然后等待，不需要在重启prometheus
     
        * k8s的json文件格式
     
          ```json
          [
            {
              "labels": {
                "job": "postgres-exporter" //当前这组目标所属的任务名称
              },
              "targets": [
                "pod-1.namespace.svc.cluster.local:9187",
                "pod-2.namespace.svc.cluster.local:9187"
              ]
            }
          ]
          ```

3. 使用exporter监控各种服务或容器

   1. 二进制安装exporter

      * 下载、解压

      * 在解压出来的的文件夹里面，创建链接服务或容器的文件`.cnf`

        ```bash
        cat >/.cnf <<"EOF"
        [client]
        user=用户名
        password=密码
        host=ip
        port=端口
        EOF
        ```

      * 创建在systemd服务管理`/etc/systemd/system/`目录下创建`.service`

        ```bash
        [Unit]
        Description=服务名称 service
        After=network.target
        
        [Service]
        ExecStart=解压文件地址\
                  -- 配置相应配置（包括用户名密码等等、和需要prometheus需要的数据）
                  # 解压文件地址 -h 查看所有可配置选项
        StandardOutput=file:/export/server/prometheus-3.4.0/prometheus.log # 重定向到日志
        KillMode=process
        
        [Install]
        WantedBy=multi-user.target
        ```

      * 启动exporter、加入开机自启

   2. 使用容器镜像形式安装exporter

      * 下载相关exporter镜像
      * 使用k8s.yml运行
      * 在yaml文件的env配置项下加入相关配置

### 配置监控postgresql

1. 拉取镜像postgresql_exporter

2. 编写yaml文件运行给定参数

   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: postgresql-exporter-deployment
   spec:
     selector:
       matchLabels:
         app: postgresql-exporter
     template:
       metadata:
         labels:
           app: postgresql-exporter
       spec:
         nodeSelector:
           kubernetes.io/hostname: k8s-node-96
         containers:
           - name: postgresql-exporter
             image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/quay.io/prometheuscommunity/postgres-exporter:latest
             resources:
               limits:
                 memory: "512Mi"
                 cpu: "500m"
             ports:
               - containerPort: 9187
                 name: postgresq-port
             env:
               - name: DATA_SOURCE_NAME
                 value: "postgresql://sonaruser:sonarpassword@qualityinspection-service.default.svc.cluster.local:5432/sonarqube?sslmode=disable" # 使用逗号分隔多个数据源，访问多个数据库，或者运行多个exporter容器
   
   ---
   apiVersion: v1
   kind: Service
   metadata:
     name: postgresql-exporter-service
   spec:
     type: NodePort
     selector:
       app: postgresql-exporter
     ports:
       - name: postgresq-port
         port: 9187
         targetPort: 9187
         nodePort: 30088
   ```
   
3. 配置`prometheus.yml`文件

   ```yaml
   - job_name: "postgresql"
       file_sd_configs:
         - files:
           -  /export/server/prometheus-3.4.0/postgresql_exporter.json
           refresh_interval: 5s
   ```

4. 动态配置json

   ```json
   [
     {
       "labels": {
         "job": "postgresql_exporter"
       },
       "targets": [
         "192.168.31.98:30088"
       ]
     }
   ]
   ```

5. 重启prometheus`systemctl restart prometheus`

## PromQL（过滤语句）

1. 基本过滤：
   * 直接写prometheus的键值（key）、直接输入
2. 根据条件过滤
   * 写入基本过滤的的查询，加上花括号`{}`里面以`key="valve"`的形式查询
   * 格式`基本过滤{需要过滤的键（属性）="键的值（想要的值）"}`
   * 多个指标在花括号内以`,`逗号进行分割开
3. 符号
   * `=`等于
   * `!=`不等于
   * `=~`支持正则表达式，匹配
   * `!~`支持正则表达式，不匹配
4. 使用函数`https://prometheus.ac.cn/docs/prometheus/latest/querying/examples/`

## Pushgateway

1. 应用场景：自定义监控
2. Pushgateway，相当于自定义的一个exporter

### 使用流程

1. 部署Pushgateway（可以在需要的机器都部署一个、也可以共用一个），启动；
2. 将Pushgateway服务端的ip域名+端口写入配置（动态或静态都行）
3. 写脚本获取需要的信息、将信息发送到Pushgateway

==**部署**==

1. 容器

   1. 拉取镜像、使用k8s.yaml运行

2. 二进制

   1. 下载、解压、使用systemctl管理、在`/etc/systemd/system/`目录下创建`.service`文件、开机自启动
   2. 解压目录下`Pushgateway -h`查看可用配置

3. 修改prometheus动态配置文件

   ```json
   [
     {
       "labels": {
         "job": "Pushgateway_exporter" //命名
       },
       "targets": [
         "192.168.31.98:30088" //Pushgateway地址端口
       ]
     }
   ]
   ```

4. 脚本：使用`curl`命令提交数据

   * 单行命令提交一次性数据`echo " key value"| curl --data-binary @- http://Pushgateway地址:端口/metrics/job/分组名（动态配置文件里面的名称）/instance/实列名字`

   * 脚本`推送当前内存使用率`

     ```bash
     #!/bin/bash
     # 设置Pushgateway服务的URL。请根据实际情况修改IP地址和端口号。
     PUSHGATEWAY_URL="http://pushgateway.example.com:9091"
     # 定义推送指标的任务名称（job）。这有助于在Prometheus中对不同任务进行分组。
     JOB_NAME="example-job"
     # 实例名（可选），用于区分来自不同来源的数据。可以根据需要更改为你的实例标识。
     INSTANCE="my-instance"
     # 指定要推送的自定义指标名称。
     METRIC_NAME="my_custom_metric"
     # 获取脚本的第一个参数作为指标值。这个值可以是任何数值类型的数据，比如系统监控数据、计算结果等。
     # 如果没有提供参数，给出使用提示并退出脚本。
     METRIC_VALUE=$1
     if [ -z "$METRIC_VALUE" ]; then
       echo "Usage: $0 <metric_value>"
       exit 1
     fi
     # 构建将要发送给Pushgateway的指标数据。这里我们创建一个名为'my_custom_metric'的gauge类型指标，
     # 并附带两个标签：job和instance，分别设置为$JOB_NAME和$INSTANCE。
     #在 Prometheus 的指标格式中，# TYPE 注释行用于声明指标的类型（例如：计数器 counter、测量值 gauge、直方图 histogram 等）
     METRIC_PAYLOAD="# TYPE $METRIC_NAME gauge
     $METRIC_NAME{job=\"$JOB_NAME\",instance=\"$INSTANCE\"} $METRIC_VALUE"
     # 使用curl命令将构建好的指标数据推送到Pushgateway。
     # --data-binary @- 参数告诉curl从标准输入读取数据（即我们的指标数据）。
     # 在URL中包含/job/$JOB_NAME/instance/$INSTANCE路径是为了确保这些标签信息能被正确识别。
     echo -e "$METRIC_PAYLOAD" | curl --data-binary @- "$PUSHGATEWAY_URL/metrics/job/$JOB_NAME/instance/$INSTANCE"
     ```

   * 必须执行这个脚本才会发送数据到Pushgateway

5. 将脚本加入定时任务

   ```bash
   crontab -e
   1 * * * * sh /目录/脚本.sh  &>/dev/null# 表示每分钟执行一次
   ```


## Grafana

* prometheus webui一般用于调试PromQL语句（查询语句）
* 展示交给Grafana。本质：Grafana 执行PromQL语句，然后通过Grafana进行展示

### 部署

1. 拉取镜像

2. yaml运行

   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: grafana-deployment
   spec:
     selector:
       matchLabels:
         app: grafana
     template:
       metadata:
         labels:
           app: grafana
       spec:
         nodeSelector:
           kubernetes.io/hostname: k8s-node-96
         containers:
           - name: grafana
             image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/quay.io/devtron/grafana:7.3.1
             resources:
               limits:
                 memory: "512Mi"
                 cpu: "500m"
             ports:
               - containerPort: 3000
                 name: grafana-port
             volumeMounts:
             - mountPath: /var/lib/grafana
               name: grafana-data
             - mountPath: /var/log/grafana
               name: grafana-log
             - mountPath: /etc/grafana/provisioning
               name: grafana-provisioning
         volumes:
         - name: grafana-data
           hostPath:
             path: /opt/grafana/lib/grafana
         - name: grafana-log
           hostPath:
             path: /opt/grafana/log/grafana
         - name: grafana-provisioning
           hostPath:
             path: /opt/grafana/provisioning
   ---
   apiVersion: v1
   kind: Service
   metadata:
     name: grafana-service
   spec:
     type: NodePort
     selector:
       app: grafana
     ports:
       - name: grafana-port
         port: 3000
         targetPort: 3000
         nodePort: 30089
   ```

3. 访问`ip加端口`

   * 初始用户名密码
     - **用户名**: `admin`
     - **密码**: `admin`

4. 添加数据源`设置---》datasources`

   * setting ：name为链接添加一个链接名称
   * dashboards：添加仪表盘（显示）

5. manag：点击需要使用的仪表盘

### 获取第三方仪表盘

1. 在仪表盘右方点击说明上方，可以remove删除
2. 点击某一个表盘上方（选择edit）或者鼠标放置在表盘上按`e`键，进入PromQL编写，在该表盘上展示什么数据
3. 第三方仪表盘地址`https://grafana.com/grafana/dashboards/?spm=5176.28103460.0.0.178d1db8nYLDik`
4. 导入
   * 在第三方找到仪表盘复制`id`或者`下载json`
   * 在grafana中点击加号

# zabbix

## 架构

![image-20250512040226783](../typoratuxiang/yunwei/zabbix/zbx1.png)

## 部署

1. 获取镜像，使用`.yaml`运行
2. 二进制安装



# Prometheus和zabbix的区别

Prometheus和Zabbix都是广泛使用的监控工具，但它们在设计理念、适用场景、数据收集方式等方面存在显著差异。以下是两者的主要区别：

1. **数据采集方式**：
   - Prometheus主要采用拉取（pull）模式，通过HTTP请求定期从配置的目标中拉取数据。这些目标需要暴露一个/metrics的HTTP端点。
   - Zabbix则通常使用推送（push）模式，Zabbix Agent会主动将监控数据推送到Zabbix Server。

2. **存储方式**：
   - Prometheus自带时间序列数据库（TSDB），专门用于存储时间序列数据，并使用WAL（写前日志）来确保数据可靠性。
   - Zabbix依赖于关系型数据库（如MySQL或PostgreSQL）来存储其监控数据。

3. **自动化与服务发现**：
   - Prometheus具有很强的服务发现能力，可以自动发现新的监控目标并进行监控，特别适合动态环境如Kubernetes。
   - Zabbix也提供了服务发现功能，但是可能需要更多的手动配置。

4. **可视化和告警**：
   - Zabbix提供了一个内置的Web界面用于查看监控数据和设置告警。
   - Prometheus本身的数据展示能力较弱，通常与Grafana结合使用以实现高级可视化。告警管理则由Alertmanager负责。

5. **性能和扩展性**：
   - Prometheus设计时考虑了大规模的时间序列数据处理，具有良好的性能和扩展性。
   - Zabbix同样支持高扩展性，特别是在分布式部署中通过Zabbix Proxy减轻主服务器负载，但在超大规模环境中可能需要更多资源和复杂配置。

6. **适用场景**：
   - Prometheus更适合云原生、容器化环境以及微服务架构的监控，因为它与Kubernetes等现代平台深度集成。
   - Zabbix更适用于需要跨多个网络或防火墙的监控环境，或者当需要监控多样化的设备和服务时，它提供的多种协议支持使其成为一个更全面的选择。
